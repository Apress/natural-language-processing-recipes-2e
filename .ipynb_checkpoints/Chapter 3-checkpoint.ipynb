{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text to features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-1. Converting Text to Features Using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Generating the features\n",
    "\n",
    "pd.get_dummies(Text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-2. Converting Text to Features Using Count Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the function \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Text\n",
    "\n",
    "text = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "\n",
    "# create the transform\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenizing\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "\n",
    "# encode document\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize & generating output\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-3. Generating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Text = \"I am learning NLP\"\n",
    "\n",
    "#Import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#For unigram : Use n = 1\n",
    "\n",
    "TextBlob(Text).ngrams(1)\n",
    "\n",
    "#For Bigram : For bigrams, use n = 2 \n",
    "\n",
    "TextBlob(Text).ngrams(2)\n",
    "\n",
    "#importing the function \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Text\n",
    "\n",
    "text = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "\n",
    "# create the transform\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# tokenizing\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# encode document\n",
    "\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize & generating output\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nltk\n",
    "from nltk import bigrams    \n",
    "import itertools\n",
    "\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_to_index = { word:i for i, word in enumerate(vocab) }\n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_to_index[current]\n",
    "        pos_previous = vocab_to_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count \n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix,vocab_to_index\n",
    "\n",
    "# sentences for testing\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'love','to' 'learn'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'is', 'cool']]\n",
    "\n",
    "# create one list using many lists\n",
    "\n",
    "merged = list(itertools.chain.from_iterable(sentences))\n",
    "matrix = co_occurrence_matrix(merged)\n",
    "\n",
    "# generate the matrix\n",
    "\n",
    "CoMatrixFinal = pd.DataFrame(matrix[0], index=vocab_to_index, columns=vocab_to_index)\n",
    "print(CoMatrixFinal)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Transform\n",
    "vectorizer = HashingVectorizer(n_features=10)\n",
    "\n",
    "# create the hashing vector\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize the vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray()) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-6. Converting Text to Features Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 6, 'jumped': 3, 'the': 7, 'over': 5, 'brown': 0, 'fox': 2, 'dog': 1, 'lazy': 4}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "Text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "\n",
    "#Import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Create the transform\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Tokenize and build vocab\n",
    "\n",
    "vectorizer.fit(Text)\n",
    "\n",
    "#Summarize\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-7. Implementing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "#import library \n",
    "\n",
    "!pip install gensim\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# training the model\n",
    "\n",
    "skipgram = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 1)\n",
    "print(skipgram)\n",
    "\n",
    "# access vector for one word\n",
    "\n",
    "print(skipgram['nlp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access vector for another one word\n",
    "\n",
    "print(fast['deep'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "skipgram.save('skipgram.bin')\n",
    "\n",
    "# load model\n",
    "\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T – SNE plot \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import library \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Example sentences\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "# training the model\n",
    "\n",
    "skipgram = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 1)\n",
    "print(skipgram)\n",
    "\n",
    "# access vector for one word\n",
    "\n",
    "print(skipgram['nlp'])\n",
    "\n",
    "# save model\n",
    "\n",
    "skipgram.save('skipgram.bin')\n",
    "\n",
    "# load model\n",
    "\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T – SNE plot \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim package\n",
    "\n",
    "import gensim\n",
    "\n",
    "# load the saved model \n",
    " \n",
    "model = gensim.models.Word2Vec.load_word2vec_format('C:\\\\Users\\\\GoogleNews-vectors-negative300.bin', binary=True)  \n",
    " \n",
    "\n",
    "#Checking how similarity works. \n",
    "\n",
    "print (model.similarity('this', 'is'))\n",
    "\n",
    "#Lets check one more.\n",
    "print (model.similarity('post', 'book'))\n",
    "\n",
    "# Finding the odd one out.\n",
    "\n",
    "model.doesnt_match('breakfast cereal dinner lunch';.split())\n",
    "\n",
    "# It is also finding the relations between words. \n",
    "\n",
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-8 Implementing FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import FastText\n",
    "\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Example sentences\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "fast = FastText(sentences,size=20, window=1, min_count=1, workers=5, min_n=1, max_n=2)\n",
    "\n",
    "# vector for word nlp\n",
    "\n",
    "print(fast['nlp'])\n",
    "print(fast['deep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "fast = Word2Vec.load('fast.bin')\n",
    "\n",
    "# visualize \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recipe 3-9. Converting Text to Features Using State-of-the-art Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "ltk.download('wordnet')\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # used for preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt # our main display package\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "df = pd.read_csv('Ted talks.csv')\n",
    "\n",
    "df_sample=df.iloc[0:100,:]\n",
    "\n",
    "remove urls, handles, and the hashtag from hashtags \n",
    "def remove_urls(text):\n",
    "    new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return new_text\n",
    "# make all text lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "# remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "    return text\n",
    "# lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return text\n",
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    text = text_lowercase(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "#preprocessing input\n",
    "for i in range(df_sample.shape[0]):\n",
    "    df_sample['description'][i]=preprocessing(str(df_sample['description'][i])) \n",
    "#in case if description has next line character\n",
    "for text in df_sample.description:\n",
    "    text=text.replace('\\n',' ')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GloVe:\n",
    "#loading pre-trained glove model\n",
    "#downloading and unzipping all word embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip   \n",
    "!unzip glove*.zip\n",
    "!ls\n",
    "!pwd\n",
    "\n",
    "#importing 100-d glove model\n",
    "glove_model_100vec = pd.read_table(\"glove.6B.100d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# getting mean vector for each sentence\n",
    "\n",
    "def get_mean_vector(glove_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    #assuming 100-d vector\n",
    "    words = [word for word in word_tokenize(words) if word in list(glove_model_100vec.index)] #if word is in vocab \n",
    "    if len(words) >= 1:\n",
    "        return np.mean(glove_model_100vec.loc[words].values, axis=0)\n",
    "    else:\n",
    "        return np.array([0]*100)\n",
    "\n",
    "#creating empty list and appending all mean arrays for comparing cosine similarities\n",
    "glove_vec=[]                                   \n",
    "for i in df_sample.description:\n",
    "    glove_vec.append(list(get_mean_vector(glove_model_100vec, i)))    \n",
    "\n",
    "glove_vec=np.asarray(glove_vec)\n",
    "glove_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ELMo:\n",
    "# Due to some open issue with TensorFlow Hub on latest version (2.x), we are degrading to tensorflow 1.x version\n",
    "#!pip uninstall tensorflow\n",
    "!pip install tensorflow==1.15\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(tf.__version__)\n",
    "\n",
    "#Load pre-trained model\n",
    "embed_ = hub.Module(\"https://tfhub.dev/google/elmo/3\")\n",
    "\n",
    "#function to average word vectors of each sentence\n",
    "def elmo_vectors_sentence(x):\n",
    "  sentence_embeddings = embed_(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # Average of each vector\n",
    "    return sess.run(tf.reduce_mean(sentence_embeddings,1))\n",
    "\n",
    "#if your data set is large , make a batch of 100 samples. Just remove comment and run the code given below. As we have just 100 samples, we are not doing this\n",
    " #samples= [df[i:i+100] for i in range(0,df.shape[0],100)]\n",
    "# elmo_vec = [elmo_vectors_sentence(x['description']) for x in samples]\n",
    " #elmo_vec_full= np.concatenate(elmo_vec, axis = 0)\n",
    "\n",
    "#embeddings on our dataset\n",
    "elmo_vec = elmo_vectors_sentence(df_sample['description'])\n",
    "\n",
    "elmo_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doc2Vec:\n",
    "\n",
    "#importing doc2vec and tagged document\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  \n",
    "\n",
    "#tokenizing data\n",
    "tokenized_data=[word_tokenize(word) for word in df_sample.description]  \n",
    "\n",
    "train_data=[TaggedDocument(d, [i]) for i, d in enumerate(tokenized_data)]   #adding paragraph id as mentioned in explanation for training\n",
    "train_data\n",
    "\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(train_data, vector_size = 100, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "def get_vectors(model,words):\n",
    "  words = [word for word in word_tokenize(words) if word in list(model.wv.vocab)]\n",
    "  #words = [word for word in word_tokenize(words) if word in list(model.wv.index_to_key)] #if gensim version is >4.0.0 ,use this line \n",
    "  if len(words)>=1:\n",
    "    return model.infer_vector(words)\n",
    "  else:\n",
    "    return np.array([0]*100) \n",
    "\n",
    "#defining empty list\n",
    "doc2vec_vec=[]                                     \n",
    "for i in df_sample.description:\n",
    "    doc2vec_vec.append(list(get_vectors(model, i)))   \n",
    "\n",
    "doc2vec_vec=np.asarray(doc2vec_vec)\n",
    "\n",
    "doc2vec_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentence-BERT:\n",
    "\n",
    "#BERT sentence transformer for sentence encoding\n",
    "!pip install sentence-transformers\n",
    "\n",
    "#importing bert-base model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "#one more model to try\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L12-v2')  \n",
    "\n",
    "#embeding on description column\n",
    "sentence_embeddings_BERT = sbert_model.encode(df_sample['description'])\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings_BERT[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Universal Encoder:\n",
    "#Load the pre-trained model\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model_USE = hub.load(module_url)\n",
    "\n",
    "\n",
    "embeddings_USE = model_USE(df_sample['description'])\n",
    "\n",
    "embeddings_USE = tf.reshape(embeddings_USE,[100,512])\n",
    "\n",
    "embeddings_USE.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Infersent:\n",
    "#There are 2 versions of InferSent. Version 1 uses GloVe while version 2 uses fastText vectors. You can choose to work with any model (we have used version 2). Thus, we download the InferSent Model and the pre-trained Word Vectors.\n",
    "\n",
    "! mkdir encoder\n",
    "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "  \n",
    "! mkdir GloVe\n",
    "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "! unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "\n",
    "\n",
    "! unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "\n",
    "\n",
    "from models import InferSent\n",
    "import torch\n",
    "\n",
    "V = 2\n",
    "MODEL_PATH = '/content/drive/MyDrive/yolov3/encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "model_infer = InferSent(params_model)\n",
    "model_infer.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = '/content/drive/MyDrive/yolov3/GloVe/glove.840B.300d.txt'\n",
    "model_infer.set_w2v_path(W2V_PATH)\n",
    "\n",
    "#building vocabulary \n",
    "model_infer.build_vocab(df_sample.description, tokenize=True)\n",
    "\n",
    "#encoding sample dataset \n",
    "\n",
    "infersent_embed = model_infer.encode(df_sample.description,tokenize=True)\n",
    "\n",
    "#shape of our vector\n",
    "infersent_embed.shape \n",
    "\n",
    "get_embed(df_sample,'infersent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open-AI GPT:\n",
    "#installing necessary model\n",
    "!pip install pytorch_pretrained_bert\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    " \n",
    "tokenizer_openai = OpenAIGPTTokenizer.from_pretrained('openai-gpt')  #Construct a GPT Tokenizer. Based on Byte-Pair-Encoding with the following peculiarities:\n",
    "\n",
    "model_openai = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "model_openai.eval()\n",
    "print('Model Loaded')\n",
    "\n",
    "#function to get embedding of each token\n",
    "def Embedding_openai(Sentence):\n",
    "  tokens = word_tokenize(Sentence)\n",
    "  vectors = np.zeros((1,768))\n",
    "  for word in tokens:\n",
    "      subwords = tokenizer_openai.tokenize(word)\n",
    "      indexed_tokens = tokenizer_openai.convert_tokens_to_ids(subwords)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      with torch.no_grad():\n",
    "          try:\n",
    "            vectors += np.array(torch.mean(model_openai(tokens_tensor),1))\n",
    "          except Exception as ex:\n",
    "            continue\n",
    "  vectors /= len(tokens)\n",
    "  return vectors\n",
    "\n",
    "\n",
    "# Initialize Matrix with dimension of numberof rows*vector dimension\n",
    "open_ai_vec = np.zeros((df_sample.shape[0], 768))\n",
    "\n",
    "# generating sentence embedding for each row\n",
    "for iter in range(df_sample.shape[0]):\n",
    "    text = df_sample.loc[iter,'description']\n",
    "    open_ai_vec[iter] = Embedding_openai(text)\n",
    "\n",
    "\n",
    "open_ai_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes input as dataframe and embedding model name as mentioned in function\n",
    "\n",
    "def get_embed(df,model):  \n",
    "\n",
    "  if model=='Glove':\n",
    "    return glove_vec\n",
    "  if model=='ELMO':\n",
    "    return elmo_vec\n",
    "  if model=='doc2vec':\n",
    "    return doc2vec_vec\n",
    "  if model=='sentenceBERT':\n",
    "    return sentence_embeddings_BERT\n",
    "  if model=='USE':\n",
    "    return embeddings_USE\n",
    "  if model=='infersent':\n",
    "    return infersent_embed \n",
    "  if model=='Open-ai':\n",
    "    return open_ai_vec   \n",
    "  \n",
    "get_embed(df_sample,'ELMO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
