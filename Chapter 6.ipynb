{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-1. Retrieving Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np \n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy \n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly taking sentences from internet \n",
    "\n",
    "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ] \n",
    "     \n",
    "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
    "\n",
    "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\n",
    "\n",
    "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]\n",
    "\n",
    "# Put all the documents in one list\n",
    "\n",
    "fin= Doc1+Doc2+Doc3+Doc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "#load the model\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#Preprocessing \n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' \n",
    "    text = re.sub(pattern, '', ''.join(text))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
    "\n",
    "def get_embedding(word):\n",
    "    if word in model.wv.vocab:\n",
    "        return model[x]\n",
    "    else:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting average vector for each document \n",
    "out_dict =  {}\n",
    "for sen in fin:\n",
    "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
    "    dict = { sen : (average_vector) }\n",
    "    out_dict.update(dict)\n",
    "\n",
    "# Function to calculate the similarity between the query vector and document vector\n",
    "\n",
    "def get_sim(query_embedding, average_vector_doc):\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
    "    return sim\n",
    "\n",
    "# Rank all the documents based on the similarity to get relevant docs\n",
    "\n",
    "def Ranked_documents(query):\n",
    "    query_words =  (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
    "    rank = []\n",
    "    for k,v in out_dict.items():\n",
    "        rank.append((k, get_sim(query_words, v)))\n",
    "    rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
    "    print('Ranked Documents :')\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the IR function with a query\n",
    "\n",
    "Ranked_documents(\"cricket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let’s take one more example as may be driving. \n",
    "\n",
    "Ranked_documents(\"driving\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-2. Classifying Text with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read file\n",
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#check sample content in the email\n",
    "file_content['v2'][1]\n",
    "\n",
    "#Import library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove stop words\n",
    "stop = stopwords.words('english')\n",
    "file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Delete unwanted columns\n",
    "Email_Data = file_content[['v1', 'v2']]\n",
    "\n",
    "# Rename column names\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
    "Email_Data.head()\n",
    "\n",
    "#Delete punctuations, convert text in lower case and delete the double space \n",
    "\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub(' ', ' ', x))\n",
    "Email_Data['Email'].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating text(input) and target classes\n",
    "\n",
    "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
    "list_classes = [\"Target\"]\n",
    "target = Email_Data[list_classes].values\n",
    "\n",
    "\n",
    "To_Process=Email_Data[['Email', 'Target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train and test split with 80:20 ratio\n",
    "train, test = train_test_split(To_Process, test_size=0.2) \n",
    "\n",
    "# Define the sequence lengths, max number of words and embedding dimensions\n",
    "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300 \n",
    "\n",
    "# Top 20000 frequently occurring words\n",
    "MAX_NB_WORDS = 20000 \n",
    " \n",
    "# Get the frequently occurring words\n",
    " tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "tokenizer.fit_on_texts(train.Email) \n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index \n",
    "# print(tokenizer.word_index) \n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index)) \n",
    "\n",
    "# get only the top frequent words on train\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']\n",
    "\n",
    "#import library\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# converts the character array to numeric array. Assigns levels to unique labels.\n",
    "\n",
    "le = LabelEncoder() \n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)\n",
    "\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training CNN 1D model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    " optimizer='rmsprop',\n",
    " metrics=['acc'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "\n",
    "predicted=model.predict(test_data)\n",
    "predicted\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "\n",
    "#model training\n",
    "\n",
    "print('Training SIMPLERNN model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "\n",
    "print('Training LSTM model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm\n",
    "\n",
    "#model evaluation \n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_lstm.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "\n",
    "print('Training Bidirectional LSTM model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=3,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "\n",
    "predicted_blstm=model.predict(test_data)\n",
    "predicted_blstm\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_blstm.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_blstm.round()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-3. Next word/sequence of words suggestion – Next word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Just selecting emails and connverting it into list\n",
    "Email_Data = file_content[[ 'v2']]\n",
    "\n",
    "list_data = Email_Data.values.tolist()\n",
    "list_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy \n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting list to string\n",
    "from collections import Iterable\n",
    "\n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "\n",
    "TextData=list(flatten(list_data))  \n",
    "TextData = ''.join(TextData) \n",
    "\n",
    "# Remove unwanted lines and converting into lower case\n",
    "TextData = TextData.replace('\\n','')\n",
    "TextData = TextData.lower() \n",
    "\n",
    "pattern = r'[^a-zA-z0-9\\s]' \n",
    "TextData = re.sub(pattern, '', ''.join(TextData)) \n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "tokens = tokenizer.tokenize(TextData)\n",
    "tokens = [token.strip() for token in tokens] \n",
    "\n",
    "# get the distinct words and sort it\n",
    "\n",
    "word_counts = collections.Counter(tokens)\n",
    "word_c = len(word_counts)\n",
    "print(word_c)\n",
    "\n",
    "distinct_words = [x[0] for x in word_counts.most_common()]\n",
    "distinct_words_sorted = list(sorted(distinct_words)) \n",
    "\n",
    "\n",
    "# Generate indexing for all words\n",
    "\n",
    "word_index = {x: i for i, x in enumerate(distinct_words_sorted)} \n",
    "\n",
    "\n",
    "# decide on sentence lenght\n",
    "\n",
    "sentence_length = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the dataset of input to output pairs encoded as integers\n",
    "# Generate the data for the model\n",
    "\n",
    "#input = the input sentence to the model with index \n",
    "#output = output of the model with index\n",
    "\n",
    "InputData = []\n",
    "OutputData = []\n",
    "\n",
    "for i in range(0, word_c - sentence_length, 1):\n",
    "    X = tokens[i:i + sentence_length]\n",
    "    Y = tokens[i + sentence_length]\n",
    "    InputData.append([word_index[char] for char in X])\n",
    "    OutputData.append(word_index[Y])\n",
    "\n",
    "print (InputData[:1])\n",
    "print (\"\\n\")\n",
    "print(OutputData[:1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate  X \n",
    "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
    "\n",
    "\n",
    "# One hot encode the output variable\n",
    "Y = np_utils.to_categorical(OutputData) \n",
    "\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    " \n",
    "#define the checkpoint\n",
    "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_name_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint] \n",
    "\n",
    "#fit the model\n",
    "model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "file_name = \"weights-improvement-05-6.8213.hdf5\"\n",
    "model.load_weights(file_name)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating random sequence\n",
    "start = numpy.random.randint(0, len(InputData))\n",
    "input_sent = InputData[start]\n",
    "\n",
    "# Generate index of the next word of the email \n",
    "\n",
    "X = numpy.reshape(input_sent, (1, len(input_sent), 1))\n",
    "predict_word = model.predict(X, verbose=0)\n",
    "index = numpy.argmax(predict_word)\n",
    "\n",
    "print(input_sent)\n",
    "print (\"\\n\")\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert these indexes back to words\n",
    "\n",
    "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
    "result = word_index_rev[index]\n",
    "sent_in = [word_index_rev[value] for value in input_sent]\n",
    "\n",
    "print(sent_in)\n",
    "print (\"\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recipe 6-4. Stackoverflow question recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Raw Data set link: \n",
    "https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/data?select=train-sample.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing necessary liabraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix, hstack,csr_matrix\n",
    "\n",
    "\n",
    "#importing training data\n",
    "df=pd.read_csv('train-sample.csv')\n",
    "\n",
    "#counting all null values\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "#Replacing missing values with blank space\n",
    "\n",
    "df['Tag1']=df['Tag1'].replace(np.NaN,'')\n",
    "df['Tag2']=df['Tag2'].replace(np.NaN,'')\n",
    "df['Tag3']=df['Tag3'].replace(np.NaN,'')\n",
    "df['Tag4']=df['Tag4'].replace(np.NaN,'')\n",
    "df['Tag5']=df['Tag5'].replace(np.NaN,'')\n",
    "\n",
    "#converting column type into string \n",
    "\n",
    "df['Title']=df['Title'].astype(str)  \n",
    "df['BodyMarkdown']=df['BodyMarkdown'].astype(str) \n",
    "\n",
    "#checking top 10 most common words from the Body column\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"BodyMarkdown\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)  #top 10 common words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tokenizer for sentence tokenization\n",
    "token=ToktokTokenizer()\n",
    "\n",
    "\n",
    "#stop words removing function \n",
    "def stopWords(text):\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))   #importing stopwords dictionary\n",
    "    #text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words=token.tokenize(text)                   # tokenizing sentences\n",
    "    \n",
    "    \n",
    "    filtered = [w for w in words if not w in stop_words] #filtering words which are not in stopwords\n",
    "    \n",
    "    return ' '.join(map(str, filtered))  #creating string combining all filtered words\n",
    "\n",
    "#function to remove punctuations\n",
    "def remove_punctuations(text):    \n",
    "    punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'   #list of punctuation marks\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')   #replacing punctuation mark with blank space\n",
    "    return text\n",
    "\n",
    "#function to remove frequent words but they were mostly from stopwords \n",
    "\n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):     \n",
    " \n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "    \n",
    "#cleaning the text \n",
    "   \n",
    "def clean_text(text):    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)x`\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "#URL removing function\n",
    "def remove_urls(text):  \n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "#function to remove html tag and replacing with blank space\n",
    "def remove_html(text):  \n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "#Applying all preprocessing steps defined above on both Body \n",
    "\n",
    "df['BodyMarkdown']=df['BodyMarkdown'].apply(lambda x: clean_text(x))\n",
    "df['BodyMarkdown'] = df['BodyMarkdown'].apply(remove_punctuations)\n",
    "df['BodyMarkdown'] = df['BodyMarkdown'].apply(remove_urls)\n",
    "df['BodyMarkdown'] = df['BodyMarkdown'].apply(remove_html)\n",
    "df['BodyMarkdown'] = df['BodyMarkdown'].apply(lambda x:stopWords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing TFIDF vector as tfidf_vectorizer \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#applying tfidf on Body  column\n",
    "\n",
    "tfidf_matrix2 = tfidf_vectorizer.fit_transform(df['BodyMarkdown'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip\n",
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sample data set with 100 rows for testing. Comment this line to run it on the whole dataset.\n",
    "\n",
    "dfg=df.iloc[0:100,:]\n",
    "\n",
    "# load the glove model \n",
    "\n",
    "glove_model = pd.read_table(\"glove.6B.100d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# getting mean vector for each sentence\n",
    "\n",
    "def get_mean_vector(glove_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in word_tokenize(words) if word in list(glove_model.index)] #if word is in vocab \n",
    "    if len(words) >= 1:\n",
    "        return np.mean(glove_model.loc[words].values, axis=0)\n",
    "    else:\n",
    "        return np.array([0]*100)\n",
    "\n",
    "\n",
    "#Defining empty list and appending array to the list\n",
    "\n",
    "glove_embeddings=[]                                     \n",
    "for i in dfg.BodyMarkdown:\n",
    "    glove_embeddings.append(list(get_mean_vector(glove_model, i)))    \n",
    "\n",
    "glove_embeddings_t=pd.DataFrame(K1).transpose()    \n",
    "glove_embeddings_t.to_csv('glove-vec.csv')\n",
    "\n",
    "#Loading our pre-trained vectors of each abstract\n",
    "\n",
    "K=pd.read_csv('glove-vec.csv')  \n",
    "glove_embeddings_loaded=[]                          \n",
    "\n",
    "#transforming data frame into a required array-#like structure as we did in the above step\n",
    "\n",
    "for i in range(dfg.shape[0]):\n",
    "    glove_embeddings_loaded.append(K[str(i)].values)\n",
    "glove_embeddings_loaded=np.asarray(glove_embeddings_loaded)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT\n",
    "\n",
    "!pip install pytorch_pretrained_bert\n",
    "\n",
    "# importing necessary libraries for GPT\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    " \n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "model = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "print('Model Loaded')\n",
    "\n",
    "\n",
    "#function to get embedding of each token\n",
    "def returnEmbedding(pSentence):\n",
    "  tokens = pSentence.split(' ')\n",
    "  hidden_states = np.zeros((1,768))\n",
    "  for token in tokens:\n",
    "      subwords = tokenizer.tokenize(token)\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(subwords)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "      with torch.no_grad():\n",
    "          try:\n",
    "            hidden_states += np.array(torch.mean(model(tokens_tensor),1))\n",
    "          except Exception as ex:\n",
    "            continue\n",
    "  hidden_states /= len(tokens)\n",
    "  return hidden_states\n",
    "\n",
    "\n",
    "# Initialize Matrix with number of dataset records as rows and 768 columns as embedding dimension\n",
    "X = np.zeros((df_gpt.shape[0], 768))\n",
    "\n",
    "# Generate sentence level embedding by calculating average of all word embedding\n",
    "for iter in range(df_gpt.shape[0]):\n",
    "    text = df_gpt.loc[iter,'BodyMarkdown']\n",
    "    #print(iter)\n",
    "    X[iter] = returnEmbedding(text)\n",
    "\n",
    "embeddings_GPT = X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "# Install BERT sentence transformer for sentence encoding\n",
    "!pip install sentence-transformers\n",
    "\n",
    "#running on 100 rows only for testing. Later comment this line\n",
    "df_bert=df.iloc[0:100,:] \n",
    "\n",
    "\n",
    "#importing bert-base model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "#embeding on Body column\n",
    "sentence_embeddings = sbert_model.encode(df['BodyMarkdown'])\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to derive cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cos_sim(a,b):\n",
    "\n",
    "    return dot(a, b)/(norm(a)*norm(b)) \n",
    "\n",
    "\n",
    "\n",
    "#Function which returns Top N similar sentence from data frame directly\n",
    "\n",
    "def top_n(user,p,df):    \n",
    "    \n",
    "    #Converting cosine similarities of overall data set with input queries into LIST\n",
    "    x=cosine_similarity(user,p).tolist()[0]\n",
    "    \n",
    " #store list in temp file to retrieve index\n",
    "    tmp=list(x)\n",
    "    \n",
    " #sort the list \n",
    "    x.sort(reverse=True)\n",
    " \n",
    "    print( x[0:5])\n",
    "\n",
    "    \n",
    " #get index of top 5\n",
    "    L=[]\n",
    "    for i in x[0:5]:\n",
    "    \n",
    "        L.append(tmp.index(i))\n",
    "    return df.iloc[L, [6,7]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre-process and extract embeddings for the user input text \n",
    "\n",
    "def user_transform(query,model):    \n",
    "    query= clean_text(query)\n",
    "    query= remove_punctuations(query)\n",
    "    query= remove_urls(query)\n",
    "    query= remove_html(query)\n",
    "    query= stopWords(query)\n",
    "    print(query)\n",
    "    if model=='TFIDF':\n",
    "      k=tfidf_vectorizer.transform([str(query)])\n",
    "    elif model=='BERT':\n",
    "      k=sbert_model.encode(str(query))\n",
    "    elif model=='glove_model':\n",
    "      k=get_mean_vector(glove_model,query)\n",
    "      k=k.reshape(1,-1)\n",
    "    elif model=='GPT':\n",
    "      k=returnEmbedding(query)  \n",
    "\n",
    "    return k\n",
    "\n",
    "    \n",
    " pd.set_option(\"display.max_colwidth\", -1)       #this function will display full text from each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=user_transform('do we have any other Q&A platform like stackoverflow which is free source?','TFIDF')   \n",
    "\n",
    "top_n(input,tfidf_matrix2,df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 5 similar questions using Glove model \n",
    "\n",
    "input=user_transform('do we have any other Q&A platform like stackoverflow which is free source?','glove_model')   #query\n",
    "\n",
    "top_n(input,glove_embeddings_loaded,df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#similar questions from GPT (from 100 rows)\n",
    "\n",
    "input=user_transform('do we have any other Q&A platform like stackoverflow which is free source?','GPT')   #query\n",
    "\n",
    "top_n(input,embeddings_GPT,df)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar questions from BERT \n",
    "\n",
    "input=user_transform('do we have any other Q&A platform like stackoverflow which is free source?','BERT')   #query\n",
    "\n",
    "top_n(input,sentence_embeddings,df)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
